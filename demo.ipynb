{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-for-Sentence-Classification-in-Chainer\n",
    "\n",
    "Implementation of Yoon Kim's [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) with Chainer.\n",
    "\n",
    "> Abstract (from Cornell university library)\n",
    ">We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.\n",
    "\n",
    "![](./img/structure.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from '/Users/atsuya/Documents/CNN-for-Sentence-Classification-in-Chainer/models.py'>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import cnn_sentence\n",
    "import data_builder\n",
    "import models\n",
    "importlib.reload(cnn_sentence)\n",
    "importlib.reload(data_builder)\n",
    "importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read Files..: 100%|██████████| 2000/2000 [00:04<00:00, 404.22it/s]\n",
      "Padding: 100%|██████████| 2000/2000 [00:00<00:00, 27393.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Info imdb\n",
      "------------------------------\n",
      "Vocab: 40666\n",
      "Sentences: 2000\n",
      "------------------------------\n",
      "x_train: (1000, 1, 2460)\n",
      "x_test: (1000, 1, 2460)\n",
      "y_train: (1000,)\n",
      "y_test: (1000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load imdb data\n",
    "data = data_builder.load_imdb_data()\n",
    "print(data.get_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['assume', 'nothing', 'the', ..., '<PAD/>', '<PAD/>', '<PAD/>'],\n",
       "       ['plot', 'derek', 'zoolander', ..., '<PAD/>', '<PAD/>', '<PAD/>'],\n",
       "       ['i', 'actually', 'am', ..., '<PAD/>', '<PAD/>', '<PAD/>'],\n",
       "       ...,\n",
       "       ['coinciding', 'with', 'the', ..., '<PAD/>', '<PAD/>', '<PAD/>'],\n",
       "       ['and', 'now', 'the', ..., '<PAD/>', '<PAD/>', '<PAD/>'],\n",
       "       ['battlefield', 'long', 'boring', ..., '<PAD/>', '<PAD/>',\n",
       "        '<PAD/>']], dtype='<U25')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[   20,     5,  2576, ...,     0,     0,     0]],\n",
       "\n",
       "       [[33285,   209,  2054, ...,     0,     0,     0]],\n",
       "\n",
       "       [[   64,   300,  1090, ...,     0,     0,     0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2828,    30, 13853, ...,     0,     0,     0]],\n",
       "\n",
       "       [[ 3126,     5,   672, ...,     0,     0,     0]],\n",
       "\n",
       "       [[  403,     5,    37, ...,     0,     0,     0]]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       elapsed_time  main/loss   validation/main/loss  main/accuracy  validation/main/accuracy\n",
      "\u001b[J1           273.554       0.850728    0.694556              0.506836       0.501758                  \n"
     ]
    }
   ],
   "source": [
    "# build cnn model\n",
    "model = L.Classifier(models.cnn[\"CNN_rand\"]([3, 8, 12], data.n_vocab))\n",
    "\n",
    "train, test = data.get_chainer_dataset()\n",
    "train_iter = chainer.iterators.SerialIterator(train, 64)\n",
    "test_iter = chainer.iterators.SerialIterator(test, 64, repeat=False, shuffle=False)\n",
    "optimizer = chainer.optimizers.Adam().setup(model)\n",
    "updater = training.StandardUpdater(train_iter, optimizer, device=-1)\n",
    "\n",
    "# build trainer\n",
    "trainer = training.Trainer(updater, (5, 'epoch'), out='result')\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, device=-1))\n",
    "trainer.extend(extensions.snapshot(), trigger=(20, 'epoch'))\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(\n",
    "['epoch', 'elapsed_time', 'main/loss', 'validation/main/loss',\n",
    " 'main/accuracy', 'validation/main/accuracy']))\n",
    "\n",
    "chainer.config.train = True\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
